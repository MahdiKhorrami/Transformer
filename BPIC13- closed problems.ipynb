{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ad8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import argparse\n",
    "import pm4py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics \n",
    "\n",
    "from processtransformer import constants\n",
    "from processtransformer.models import transformer\n",
    "from processtransformer.data.loader import LogsDataLoader\n",
    "from processtransformer.data.processor import LogsDataProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed46a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eb75f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08eca50c6ec04da887d1be25e2864e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/1487 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = \"./datasets/\"\n",
    "if not os.path.exists(data_dir): \n",
    "  os.mkdir(data_dir)\n",
    "_ = wget.download(\"https://data.4tu.nl/file/1987a2a6-9f5b-4b14-8d26-ab7056b17929/8b99119d-9525-452e-bc8f-236ac76fa9c9\")\n",
    "\n",
    "\n",
    "input_file_path= wget.download(\"https://data.4tu.nl/file/1987a2a6-9f5b-4b14-8d26-ab7056b17929/8b99119d-9525-452e-bc8f-236ac76fa9c9\")\n",
    "output_file_path='./datasets/BPIC13.csv'\n",
    "\n",
    "#Write to Pandas Dataframe\n",
    "log = pm4py.read_xes(input_file_path) #Input Filename\n",
    "df = pm4py.convert_to_dataframe(log)\n",
    "df.to_csv(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e074403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org:group</th>\n",
       "      <th>resource country</th>\n",
       "      <th>organization country</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>organization involved</th>\n",
       "      <th>org:role</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>impact</th>\n",
       "      <th>product</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>case:concept:name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Org line A2</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>se</td>\n",
       "      <td>Minnie</td>\n",
       "      <td>J11 2nd</td>\n",
       "      <td>A2_2</td>\n",
       "      <td>Queued</td>\n",
       "      <td>High</td>\n",
       "      <td>PROD191</td>\n",
       "      <td>Awaiting Assignment</td>\n",
       "      <td>2006-01-11 14:49:42+00:00</td>\n",
       "      <td>1-109135791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Org line A2</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>se</td>\n",
       "      <td>Minnie</td>\n",
       "      <td>J11 2nd</td>\n",
       "      <td>A2_2</td>\n",
       "      <td>Accepted</td>\n",
       "      <td>High</td>\n",
       "      <td>PROD191</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>2012-03-15 10:53:52+00:00</td>\n",
       "      <td>1-109135791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Org line A2</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>se</td>\n",
       "      <td>Minnie</td>\n",
       "      <td>J11 2nd</td>\n",
       "      <td>A2_2</td>\n",
       "      <td>Accepted</td>\n",
       "      <td>High</td>\n",
       "      <td>PROD191</td>\n",
       "      <td>Assigned</td>\n",
       "      <td>2012-03-15 10:56:17+00:00</td>\n",
       "      <td>1-109135791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Org line A2</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>se</td>\n",
       "      <td>Minnie</td>\n",
       "      <td>J11 2nd</td>\n",
       "      <td>A2_2</td>\n",
       "      <td>Accepted</td>\n",
       "      <td>High</td>\n",
       "      <td>PROD191</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>2012-03-15 11:09:05+00:00</td>\n",
       "      <td>1-109135791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Org line A2</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>se</td>\n",
       "      <td>Minnie</td>\n",
       "      <td>J11 2nd</td>\n",
       "      <td>A2_2</td>\n",
       "      <td>Completed</td>\n",
       "      <td>High</td>\n",
       "      <td>PROD191</td>\n",
       "      <td>Closed</td>\n",
       "      <td>2012-03-15 11:11:33+00:00</td>\n",
       "      <td>1-109135791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6655</th>\n",
       "      <td>Org line C</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>se</td>\n",
       "      <td>Karl</td>\n",
       "      <td>G161 2nd</td>\n",
       "      <td>E_7</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Medium</td>\n",
       "      <td>PROD671</td>\n",
       "      <td>Closed</td>\n",
       "      <td>2012-05-30 13:04:30+00:00</td>\n",
       "      <td>1-752134249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6656</th>\n",
       "      <td>Org line A2</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>se</td>\n",
       "      <td>Niklas</td>\n",
       "      <td>U6 2nd</td>\n",
       "      <td>A2_3</td>\n",
       "      <td>Accepted</td>\n",
       "      <td>Major</td>\n",
       "      <td>PROD831</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>2012-05-31 09:58:45+00:00</td>\n",
       "      <td>1-752600115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6657</th>\n",
       "      <td>Org line A2</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>se</td>\n",
       "      <td>Niklas</td>\n",
       "      <td>U6 2nd</td>\n",
       "      <td>A2_3</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Major</td>\n",
       "      <td>PROD831</td>\n",
       "      <td>Closed</td>\n",
       "      <td>2012-05-31 10:01:24+00:00</td>\n",
       "      <td>1-752600115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6658</th>\n",
       "      <td>Org line G3</td>\n",
       "      <td>POLAND</td>\n",
       "      <td>us</td>\n",
       "      <td>Ewa</td>\n",
       "      <td>G199 3rd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Accepted</td>\n",
       "      <td>High</td>\n",
       "      <td>PROD97</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>2012-05-31 18:07:59+00:00</td>\n",
       "      <td>1-752835764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6659</th>\n",
       "      <td>Org line G3</td>\n",
       "      <td>POLAND</td>\n",
       "      <td>us</td>\n",
       "      <td>Ewa</td>\n",
       "      <td>G199 3rd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Completed</td>\n",
       "      <td>High</td>\n",
       "      <td>PROD97</td>\n",
       "      <td>Closed</td>\n",
       "      <td>2012-05-31 18:09:21+00:00</td>\n",
       "      <td>1-752835764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6660 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        org:group resource country organization country org:resource  \\\n",
       "0     Org line A2            INDIA                   se       Minnie   \n",
       "1     Org line A2            INDIA                   se       Minnie   \n",
       "2     Org line A2            INDIA                   se       Minnie   \n",
       "3     Org line A2            INDIA                   se       Minnie   \n",
       "4     Org line A2            INDIA                   se       Minnie   \n",
       "...           ...              ...                  ...          ...   \n",
       "6655   Org line C           Sweden                   se         Karl   \n",
       "6656  Org line A2           Sweden                   se       Niklas   \n",
       "6657  Org line A2           Sweden                   se       Niklas   \n",
       "6658  Org line G3           POLAND                   us          Ewa   \n",
       "6659  Org line G3           POLAND                   us          Ewa   \n",
       "\n",
       "     organization involved org:role concept:name  impact  product  \\\n",
       "0                  J11 2nd     A2_2       Queued    High  PROD191   \n",
       "1                  J11 2nd     A2_2     Accepted    High  PROD191   \n",
       "2                  J11 2nd     A2_2     Accepted    High  PROD191   \n",
       "3                  J11 2nd     A2_2     Accepted    High  PROD191   \n",
       "4                  J11 2nd     A2_2    Completed    High  PROD191   \n",
       "...                    ...      ...          ...     ...      ...   \n",
       "6655              G161 2nd      E_7    Completed  Medium  PROD671   \n",
       "6656                U6 2nd     A2_3     Accepted   Major  PROD831   \n",
       "6657                U6 2nd     A2_3    Completed   Major  PROD831   \n",
       "6658              G199 3rd      NaN     Accepted    High   PROD97   \n",
       "6659              G199 3rd      NaN    Completed    High   PROD97   \n",
       "\n",
       "     lifecycle:transition            time:timestamp case:concept:name  \n",
       "0     Awaiting Assignment 2006-01-11 14:49:42+00:00       1-109135791  \n",
       "1             In Progress 2012-03-15 10:53:52+00:00       1-109135791  \n",
       "2                Assigned 2012-03-15 10:56:17+00:00       1-109135791  \n",
       "3             In Progress 2012-03-15 11:09:05+00:00       1-109135791  \n",
       "4                  Closed 2012-03-15 11:11:33+00:00       1-109135791  \n",
       "...                   ...                       ...               ...  \n",
       "6655               Closed 2012-05-30 13:04:30+00:00       1-752134249  \n",
       "6656          In Progress 2012-05-31 09:58:45+00:00       1-752600115  \n",
       "6657               Closed 2012-05-31 10:01:24+00:00       1-752600115  \n",
       "6658          In Progress 2012-05-31 18:07:59+00:00       1-752835764  \n",
       "6659               Closed 2012-05-31 18:09:21+00:00       1-752835764  \n",
       "\n",
       "[6660 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deeff2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = LogsDataProcessor(name='BPIC13', filepath=\"datasets/BPIC13.csv\",  \n",
    "                                    columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"], #specify the columns name containing case_id, activity name and timestamp \n",
    "                                    dir_path='datasets', pool = 4)\n",
    "data_processor.process_logs(task=constants.Task.NEXT_ACTIVITY, sort_temporally= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9054215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_loader = LogsDataLoader(name = 'BPIC13')\n",
    "\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_ACTIVITY)\n",
    "\n",
    "# Prepare training examples for next activity prediction task\n",
    "train_token_x, train_token_y = data_loader.prepare_data_next_activity(train_df, \n",
    "    x_word_dict, y_word_dict, max_case_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c88eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4d5f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "367/367 [==============================] - 19s 21ms/step - loss: 1.0130 - sparse_categorical_accuracy: 0.5387\n",
      "Epoch 2/10\n",
      "367/367 [==============================] - 8s 21ms/step - loss: 0.9382 - sparse_categorical_accuracy: 0.5555\n",
      "Epoch 3/10\n",
      "367/367 [==============================] - 7s 20ms/step - loss: 0.8833 - sparse_categorical_accuracy: 0.5753\n",
      "Epoch 4/10\n",
      "367/367 [==============================] - 8s 22ms/step - loss: 0.8656 - sparse_categorical_accuracy: 0.5721\n",
      "Epoch 5/10\n",
      "367/367 [==============================] - 8s 22ms/step - loss: 0.8658 - sparse_categorical_accuracy: 0.5771\n",
      "Epoch 6/10\n",
      "367/367 [==============================] - 8s 21ms/step - loss: 0.8619 - sparse_categorical_accuracy: 0.5755\n",
      "Epoch 7/10\n",
      "367/367 [==============================] - 8s 22ms/step - loss: 0.8583 - sparse_categorical_accuracy: 0.5712\n",
      "Epoch 8/10\n",
      "367/367 [==============================] - 9s 24ms/step - loss: 0.8589 - sparse_categorical_accuracy: 0.5800\n",
      "Epoch 9/10\n",
      "367/367 [==============================] - 10s 27ms/step - loss: 0.8547 - sparse_categorical_accuracy: 0.5780\n",
      "Epoch 10/10\n",
      "367/367 [==============================] - 8s 22ms/step - loss: 0.8484 - sparse_categorical_accuracy: 0.5891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ff2ea44c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_activity_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size,\n",
    "    output_dim=num_output)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "transformer_model.fit(train_token_x, train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1730b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 17ms/step\n",
      "5/5 [==============================] - 0s 16ms/step\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "            x_word_dict, y_word_dict, max_case_length)   \n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\")\n",
    "        k.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "k.append(i + 1)\n",
    "accuracies.append(np.mean(accuracy))\n",
    "fscores.append(np.mean(fscores))\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e954582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy across all prefixes: 0.49384244883225253\n",
      "Average f-score across all prefixes: 0.40164260847114513\n",
      "Average precision across all prefixes: 0.3861076260796069\n",
      "Average recall across all prefixes: 0.5212781404340443\n"
     ]
    }
   ],
   "source": [
    "print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "print('Average precision across all prefixes:', np.mean(precisions))\n",
    "print('Average recall across all prefixes:', np.mean(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1589b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7518d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = LogsDataProcessor(name='BPIC13', filepath=\"datasets/BPIC13.csv\",  \n",
    "                                    columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],  #specify the columns name containing case_id, activity name and timestamp \n",
    "                                    dir_path='datasets', pool = 4)\n",
    "data_processor.process_logs(task=constants.Task.NEXT_TIME, sort_temporally= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08d03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_loader = LogsDataLoader(name = 'BPIC13')\n",
    "\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.NEXT_TIME)\n",
    "\n",
    "# Prepare training examples for next time prediction task\n",
    "(train_token_x, train_time_x, train_token_y, time_scaler, y_scaler) = \\\n",
    "                                    data_loader.prepare_data_next_time(train_df, x_word_dict, max_case_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c26ddb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4cb4882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 - 20s - loss: 0.2309 - 20s/epoch - 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ff79cd730>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_next_time_model(\n",
    "        max_case_length=max_case_length, \n",
    "        vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "        epochs=epochs, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75b7952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 28ms/step\n",
      "10/10 [==============================] - 0s 22ms/step\n",
      "5/5 [==============================] - 0s 47ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, maes, mses, rmses = [],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_next_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        k.append(i)\n",
    "        maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "        mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "k.append(i + 1)\n",
    "maes.append(np.mean(maes))\n",
    "mses.append(np.mean(mses))\n",
    "rmses.append(np.mean(rmses))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46861cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE across all prefixes: 19.971987\n",
      "Average MSE across all prefixes: 435.67032\n",
      "Average RMSE across all prefixes: 20.518627\n"
     ]
    }
   ],
   "source": [
    "print('Average MAE across all prefixes:', np.mean(maes))\n",
    "print('Average MSE across all prefixes:', np.mean(mses))\n",
    "print('Average RMSE across all prefixes:', np.mean(rmses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481ac2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0035967",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = LogsDataProcessor(name='BPIC13', filepath=\"datasets/BPIC13.csv\",  \n",
    "                                    columns = [\"case:concept:name\", \"concept:name\", \"time:timestamp\"],  #specify the columns name containing case_id, activity name and timestamp \n",
    "                                    dir_path='datasets', pool = 4)\n",
    "data_processor.process_logs(task=constants.Task.REMAINING_TIME, sort_temporally= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b25fc664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_loader = LogsDataLoader(name = 'BPIC13')\n",
    "\n",
    "(train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "    vocab_size, num_output) = data_loader.load_data(constants.Task.REMAINING_TIME)\n",
    "\n",
    "# Prepare training examples for next time prediction task\n",
    "(train_token_x, train_time_x, \n",
    "    train_token_y, time_scaler, y_scaler) = data_loader.prepare_data_remaining_time(train_df, \n",
    "    x_word_dict, max_case_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feb861ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 12\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81ea2565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466/466 - 22s - loss: 0.3121 - 22s/epoch - 46ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27ff7dbd940>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train a transformer model\n",
    "transformer_model = transformer.get_remaining_time_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.LogCosh())\n",
    "\n",
    "transformer_model.fit([train_token_x, train_time_x], train_token_y, \n",
    "        epochs=epochs, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e27994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 17ms/step\n",
      "10/10 [==============================] - 0s 19ms/step\n",
      "5/5 [==============================] - 0s 17ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, maes, mses, rmses = [],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_time_x, test_y, _, _ = data_loader.prepare_data_remaining_time(\n",
    "            test_data_subset, x_word_dict, max_case_length, time_scaler, y_scaler, False)   \n",
    "\n",
    "        y_pred = transformer_model.predict([test_token_x, test_time_x])\n",
    "        _test_y = y_scaler.inverse_transform(test_y)\n",
    "        _y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "        k.append(i)\n",
    "        maes.append(metrics.mean_absolute_error(_test_y, _y_pred))\n",
    "        mses.append(metrics.mean_squared_error(_test_y, _y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(_test_y, _y_pred)))\n",
    "\n",
    "k.append(i + 1)\n",
    "maes.append(np.mean(maes))\n",
    "mses.append(np.mean(mses))\n",
    "rmses.append(np.mean(rmses))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74c295ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE across all prefixes: 117.24571\n",
      "Average MSE across all prefixes: 14280.811\n",
      "Average RMSE across all prefixes: 118.38883\n"
     ]
    }
   ],
   "source": [
    "print('Average MAE across all prefixes:', np.mean(maes))\n",
    "print('Average MSE across all prefixes:', np.mean(mses))\n",
    "print('Average RMSE across all prefixes:', np.mean(rmses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bed15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f484cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
